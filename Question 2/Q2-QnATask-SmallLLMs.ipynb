{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "DcuRAQaYFOjI"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator, pipeline\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8pgMKVHhacq"
   },
   "source": [
    "## Function for computing document similarity score using embeddings. This has been used to do retrieval from the Knowledgbase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "dce0d15538fb44689c19093d73ab9c14",
      "9d5e36a5ef184786bdfc710dff5876f2",
      "8ac60711f8ff408999e219d53554f45b",
      "71b944406dc547dab6919638b161576c",
      "6faff77815b74c08b41ec6961b3e6e31",
      "476a5ec4a37b4c9eaac259c5ae697399",
      "d1f80be4266149538b5e407db3575bf6",
      "ba30dcb7904b4a939afeea16fecdcc09",
      "344662057e6441be8f4346c7e15d18a0",
      "c2a232e5ba244fc9820a10a839222236",
      "e9099038db0145eca961e8bdeb91fb57",
      "3e144404bf6f443ca4d72e01f48d2a20",
      "0f4239009d3e4f708b769fa98836ad39",
      "5e8de19ab7e24234865e3c01e837ed48",
      "daf9a6397de4453eafe94b99599877aa",
      "11c44d6c63f74e26a23da7523cf48b23",
      "99cff9138d2740348c09cbf0287ee99f",
      "476193ea526348de95d20aab24d1d862",
      "68ba66dfd2794905b8c21ebc86556bc1",
      "106ff73e18604847af3241fd6cb0331c",
      "98a3fadfafc84de29c4c251e35050bb7",
      "f581adf271524056a0993bf8b82305d0",
      "6a4a8f70161b4b978aa6eddd01c1623c",
      "6dd7be817f2b472c9851f4a8e8cb7040",
      "e788bb1575224a54845a39ac09d72bc0",
      "121be21d69cf43a88a16773c85504d59",
      "4be1598242d5427aa5f41d06b6f8eb50",
      "c37af99238694fbfbf0fbd7201a19656",
      "3c92c18644ca45dba5ea3fc3ac532142",
      "6cb302c49f294829800d5aaa6094d3d4",
      "4e9b6557f02944d196e2d43228d87ac4",
      "b68c14a8a9eb4bb28a1e313403e237d3",
      "47348306f72f462bbb7899c86c83f0f9",
      "d1b9d1b2a45047faac15fa750525dfb0",
      "7caa75a665994c46a8cef15539fc0e4a",
      "d6826a8b310543c5ba42a7a75bab6d32",
      "1b5f4a105d4f4acc8345a1ecca5fa7d0",
      "469ef18c6d4f492c99c59fcd5e228257",
      "ac3ac027dbe54e919fa62fa68101efd8",
      "a214f74002bd4a089c65891de5c1d7b1",
      "45cba38226674765a92d077a5b3ca151",
      "768f69a442f34a6ca34d22d86adb33dc",
      "9d23db4de8044614bfb8893a2e4b0d64",
      "ab1359619b874fd7ad7a6387ffb9e998",
      "9e18cd0e007c43bea049274ee9d0bbb8",
      "efc87e285b704ddca718368fa1da4c1d",
      "10bb494dbc3c439daa23996dc9f99bc6",
      "b01b5a6c3aec4f9ea40f7e4abfe764df",
      "b5e37e2a61764d099ede48be25fe1495",
      "aa85a566987a4ccf8ecdf8e2a87e98f6",
      "93d5913d691248719fc7666c11e42c49",
      "f962531ecbe74343ae8e63fa613ec76f",
      "9bbc22b7742344a681bb035d5c598329",
      "8afdb7b3d3b4402dbc59be4290de5228",
      "6ca2b5e0ca464e049e8eb40eca2cd41e"
     ]
    },
    "id": "jdQ55za6Hd8q",
    "outputId": "c0ec4e29-9cad-4daf-a20a-b7bfe5a14c0d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce0d15538fb44689c19093d73ab9c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e144404bf6f443ca4d72e01f48d2a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4a8f70161b4b978aa6eddd01c1623c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b9d1b2a45047faac15fa750525dfb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e18cd0e007c43bea049274ee9d0bbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model_name = \"distilbert-base-uncased\"\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name)\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "\n",
    "embedding_tokenizer.pad_token = embedding_tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "def compare_documents(doc1, doc2):\n",
    "\n",
    "    # Tokenize and encode the documents\n",
    "    encoding1 = embedding_tokenizer(doc1, return_tensors='pt', truncation=True,max_length=200)\n",
    "    encoding2 = embedding_tokenizer(doc2, return_tensors='pt', truncation=True,max_length=200)\n",
    "\n",
    "    # Compute model scores\n",
    "    with torch.no_grad():\n",
    "        outputs1 = embedding_model(**encoding1)\n",
    "        outputs2 = embedding_model(**encoding2)\n",
    "\n",
    "    embeddings1 = outputs1.last_hidden_state.mean(dim=1)\n",
    "    embeddings2 = outputs2.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Compare the scores\n",
    "    similarity_score = cosine_similarity(embeddings1, embeddings2)[0][0]\n",
    "\n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zFC8X-Shr66"
   },
   "source": [
    "# Training dataset for finetuning small LLM for QnA task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VQ1fT_eAFj9W"
   },
   "outputs": [],
   "source": [
    "def _load_dataset_from_hf_for_training(name_of_dataset: str = 'squad',\n",
    "                                       initial_split: str = \"train[0:2000]\",\n",
    "                                       test_size_for_train_test_split: float = 0.25):\n",
    "\n",
    "  subset_qna_dataset = load_dataset(\"squad\", split=initial_split)\n",
    "  subset_qna_dataset_train_test_split = subset_qna_dataset.train_test_split(test_size=0.25)\n",
    "\n",
    "  return subset_qna_dataset_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BS743Vq2Fux-"
   },
   "outputs": [],
   "source": [
    "def _get_model_and_initialize(model_name: str = 'gpt2'):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "  return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgSHkrPThykR"
   },
   "source": [
    "# This is dataset preprocessing function for model training for QnA task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CqNf--P8FyHz"
   },
   "outputs": [],
   "source": [
    "model_name = \"google-t5/t5-base\"\n",
    "def _preprocess_function(examples):\n",
    "    '''\n",
    "    This preprocessing function is heavily based out of the hugging face example\n",
    "    '''\n",
    "    tokenizer, _ = _get_model_and_initialize(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    prefix = \"Answer this question based on the context: \"\n",
    "    questions = [prefix + q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(questions, examples[\"context\"], max_length=512, truncation=\"only_second\", return_offsets_mapping=True, padding=\"max_length\")\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EslZB_wQHndq",
    "outputId": "beff73ca-e89f-416c-b880-821890ac5a9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive',force_remount = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtMMXvguh45H"
   },
   "source": [
    "# Finetuning the Model based on the dataset defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sUbHJ6nOHgu_"
   },
   "outputs": [],
   "source": [
    "def finetune_model_and_save(model_name: str = 'gpt2',\n",
    "                   directory_for_model_storing:str = 'qna_gpt_bot'):\n",
    "  subset_qna_dataset_train_test_split = _load_dataset_from_hf_for_training()\n",
    "\n",
    "  tokenizer, model = _get_model_and_initialize(model_name)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "  tokenized_datasets = subset_qna_dataset_train_test_split.map(_preprocess_function, batched=True)\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "    output_dir = directory_for_model_storing,\n",
    "    logging_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=0.0001,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "\n",
    "  data_collator = DefaultDataCollator()\n",
    "  trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "  trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503,
     "referenced_widgets": [
      "94e6faca7e3f48a4816eb8eb22cf71fc",
      "4629f5d47b184d939fbf5925337d7342",
      "ee39c3acf17943958413c08a8b6359f8",
      "5c181c1a07e04cffac04b0a502087bec",
      "e2a4b97852a740a19640932a40c024c8",
      "dd9b75eaaf244f0fa830a9acefda443c",
      "a9afdfec7a204135a9fdbfdf215fcbc3",
      "13a576fdbe43460cb843f556c43462ce",
      "dbf40dbed8cc4d81a9c25b6048f118a5",
      "d61a45973d21442691a3a1bea2a43dee",
      "5a2c5553d3ad4927b20dce803b9c47d9",
      "bbaec9ede2154d698abea1673882e595",
      "461d82ec669940199332b680572254f0",
      "0bad37ba388f40088bef4c4551d8f336",
      "dd964abf630d4a69811ce5f2ea113a7d",
      "4ee4e394caf0478e8cd210ced5cda60b",
      "7637528176a94615b3f648222dfc8d91",
      "05965e18aac04abb8d6e28a869ca2ad9",
      "024ca83d661a4542975764dc77385339",
      "af815a3470cd44dba198dcb860560863",
      "fd1141f7c0f54b87b6a12960b160d7b0",
      "213230af30f44947aa03bd723a5b3088"
     ]
    },
    "id": "yJPpqpBZKU8Q",
    "outputId": "61438e7b-f42e-41cf-e943-6aa4b2a8cd66"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at gpt2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e6faca7e3f48a4816eb8eb22cf71fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at gpt2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at gpt2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbaec9ede2154d698abea1673882e595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at gpt2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 05:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.275700</td>\n",
       "      <td>2.141180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.674900</td>\n",
       "      <td>2.055543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.942700</td>\n",
       "      <td>2.276876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetune_model_and_save(model_name = 'gpt2',\n",
    "                   directory_for_model_storing = 'qna_gpt_bot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dhy1owRih-WT"
   },
   "source": [
    "# Function to get LLM response given a context and a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgTmuctHKLsZ"
   },
   "outputs": [],
   "source": [
    "def get_answer(context, question,model_path:str):\n",
    "  qna = pipeline(task=\"question-answering\", model=model_path, verbose=False)\n",
    "  prefix = \"Answer this question based on the context: \"\n",
    "  return qna(prefix + question, context)['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521,
     "referenced_widgets": [
      "e7c0c02c91a54b59996bae21eb62f544",
      "d84984b083414264a9f4fbb315a41c35",
      "a91cf0f49fb14876b9292f63b88dfbf1",
      "5ee610a2d5cb473a9bed75b23c863847",
      "96a115aff57348c081b6bafefac7be4f",
      "18a2468b119c4af0adc6629149a001c9",
      "139e5479c6824e508e2e8b753db5ae81",
      "51129b6d42a14fd0b1a89193aca76b63",
      "aacb3a955b2b4f52b94b665ba694eee5",
      "d16c9edffc634498b4360612e9bc719e",
      "119ca236d1f1424aa89dcdcb6625d934",
      "dcdb0a9c9f2144429ef39268e737db89",
      "00b2622b964746b795e595704edd2caa",
      "c7ac1aff046a456eba939fa9fc82a7c0",
      "60c706d096ac499bb1ae24cb0be4d58c",
      "eae7ba0fb43d4f7fbf8e28b9f7602a0d",
      "773a8933411a4f99a9de8c675bb26e55",
      "73b779b42782409dae8eb1085a8981f5",
      "9aa3d56799f0460f885f9c83737ba276",
      "698eceb6031e47408ecfee66f452c35f",
      "a76f180702224235b67b66d5b37af053",
      "d57e2bab46ca40679fe7ccc8f4706743"
     ]
    },
    "id": "_OMOiDCKNGLI",
    "outputId": "753c2a1d-397f-4fe5-e953-7a0ee1aa8f6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Some weights of T5ForQuestionAnswering were not initialized from the model checkpoint at google-t5/t5-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c0c02c91a54b59996bae21eb62f544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForQuestionAnswering were not initialized from the model checkpoint at google-t5/t5-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of T5ForQuestionAnswering were not initialized from the model checkpoint at google-t5/t5-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdb0a9c9f2144429ef39268e737db89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForQuestionAnswering were not initialized from the model checkpoint at google-t5/t5-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 11:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.263400</td>\n",
       "      <td>1.182161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.089500</td>\n",
       "      <td>0.972934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.696500</td>\n",
       "      <td>1.038409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    }
   ],
   "source": [
    "finetune_model_and_save(model_name = \"google-t5/t5-base\",\n",
    "                   directory_for_model_storing = 'qna_t5_bot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXO-3uXHiE0W"
   },
   "source": [
    "# Sample Knowledgebase for Retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "kJSdZc_2Njjd"
   },
   "outputs": [],
   "source": [
    "knowledgebase = [\n",
    "    \"The Pacific Ocean is the largest ocean on Earth, covering more than 63 million square miles.\",\n",
    "    \"The light bulb was invented by Thomas Edison in 1879.\",\n",
    "    \"Sharks have been around for over 400 million years, predating the dinosaurs.\",\n",
    "    \"The Eiffel Tower in Paris was completed in 1889 and is 1,083 feet tall.\",\n",
    "    \"Brazil is the fifth largest country in the world by both area and population.\",\n",
    "    \"The human skeleton consists of 206 bones.\",\n",
    "    \"Venus is the hottest planet in our solar system with surface temperatures around 900 degrees Fahrenheit.\",\n",
    "    \"The Great Wall of China stretches over 13,000 miles and was primarily built using stone, brick, and wood.\",\n",
    "    \"Coffee is the second most traded commodity in the world, after oil.\",\n",
    "    \"The longest river in the world is the Nile River, which runs for 4,135 miles.\",\n",
    "    \"Antarctica is the coldest continent on Earth, with temperatures as low as minus 128.6 degrees Fahrenheit.\",\n",
    "    \"The theory of relativity was developed by Albert Einstein in the early 20th century.\",\n",
    "    \"Australia is the only country that is also a continent.\",\n",
    "    \"Honey never spoils and has been found edible in ancient Egyptian tombs after thousands of years.\",\n",
    "    \"The loudest animal relative to its size is the water boatman, which can produce sounds up to 99.2 decibels.\",\n",
    "    \"The human brain uses approximately 20% of the bodyâ€™s energy despite making up only 2% of its weight.\",\n",
    "    \"Mount Everest is the highest point on Earth, standing at 29,029 feet above sea level.\",\n",
    "    \"Octopuses have three hearts; two pump blood to the gills, while the third pumps it to the rest of the body.\",\n",
    "    \"A day on Venus is longer than a year on Venus; it takes 243 Earth days to rotate once\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cREP6DTliMwd"
   },
   "source": [
    "# Basic version of Retriever using vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "dYfcN62JgHa-"
   },
   "outputs": [],
   "source": [
    "def get_context_from_kb(question:str) -> str:\n",
    "  context = \"\"\n",
    "\n",
    "  similarity_score = 0\n",
    "  for i in knowledgebase:\n",
    "    sim_score = compare_documents(question, i)\n",
    "    if sim_score > similarity_score:\n",
    "      context = i\n",
    "      similarity_score = sim_score\n",
    "\n",
    "  return context, similarity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhGkdHrriSad"
   },
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v6wKB2TxgXRj",
    "outputId": "2d95a52c-58b4-4f71-d325-b445f8204b93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the length of the Great Wall of China\n",
      "The Great Wall of China stretches over 13,000 miles and was primarily built using stone, brick, and wood.\n",
      "0.8406506\n"
     ]
    }
   ],
   "source": [
    "q = 'What is the length of the Great Wall of China'\n",
    "context, sim_score = get_context_from_kb(q)\n",
    "print(q,context,sim_score,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-D5EIVifplD",
    "outputId": "c1005167-b0fa-4bda-f8bc-3c5bcb1085b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13,000 miles\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(context, q,model_path='/content/qna_t5_bot/checkpoint-564'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mR9O6x9aQ1kJ",
    "outputId": "ac9604c0-d202-4e2f-c2e7-24fb6d0fe638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13,000 miles\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(context, q,model_path='/content/qna_gpt_bot/checkpoint-564'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSJmCINyiUUQ"
   },
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pIpTfu0CTzo9",
    "outputId": "8e081a57-605a-4037-d7bd-31dee3f83386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who invented light bulb\n",
      "The light bulb was invented by Thomas Edison in 1879.\n",
      "0.82907844\n"
     ]
    }
   ],
   "source": [
    "q = 'Who invented light bulb'\n",
    "context, sim_score = get_context_from_kb(q)\n",
    "print(q,context,sim_score,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWWWFOW-V6lQ",
    "outputId": "0426547c-a1d0-4c76-eca4-9f7f6e8eea20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thomas Edison\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(context, q,model_path='/content/qna_t5_bot/checkpoint-564'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lv3EMwIUVQGw",
    "outputId": "6478c308-c579-476b-e764-5a24fcf14088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Thomas Edison\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(context, q,model_path='/content/qna_gpt_bot/checkpoint-564'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSx8xMhriVyF"
   },
   "source": [
    "# Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CV4FsORghH5t",
    "outputId": "06541316-57d7-430a-d81d-f4b1086f4ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What year was the light bulb invented\n",
      "The light bulb was invented by Thomas Edison in 1879.\n",
      "0.73382187\n"
     ]
    }
   ],
   "source": [
    "q = 'What year was the light bulb invented'\n",
    "context, sim_score = get_context_from_kb(q)\n",
    "print(q,context,sim_score,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBW8uNVOhLwF",
    "outputId": "0f5a17bf-5df8-40ca-dc25-6686a19b1fb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1879.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(context, q,model_path='/content/qna_t5_bot/checkpoint-564'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLbTxNgJhMXh",
    "outputId": "4ccfe1c5-2e7e-4f2c-f00e-a636362b8ae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1879\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(context, q,model_path='/content/qna_gpt_bot/checkpoint-564'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6aWBT2WhNi3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
